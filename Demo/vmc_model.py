"""
VMC (Variational Memory Compression) æ¨¡å‹æ¶æ„
åŒ…å« V (Variational), M (Memory), C (Controller) ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from config import VMCConfig

class VariationalEncoder(nn.Module):
    """V - å˜åˆ†ç¼–ç å™¨ç»„ä»¶"""
    
    def __init__(self, input_dim, latent_dim, hidden_dims):
        super().__init__()
        self.input_dim = input_dim
        self.latent_dim = latent_dim
        
        # æ„å»ºç¼–ç å™¨ç½‘ç»œ
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.1)
            ])
            prev_dim = hidden_dim
        
        self.encoder = nn.Sequential(*layers)
        
        # å˜åˆ†å‚æ•°å±‚
        self.fc_mu = nn.Linear(prev_dim, latent_dim)
        self.fc_logvar = nn.Linear(prev_dim, latent_dim)
        
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥æ•°æ® [batch_size, input_dim]
            
        Returns:
            z: é‡å‚æ•°åŒ–é‡‡æ · [batch_size, latent_dim]
            mu: å‡å€¼ [batch_size, latent_dim]
            logvar: å¯¹æ•°æ–¹å·® [batch_size, latent_dim]
        """
        h = self.encoder(x)
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        
        # é‡å‚æ•°åŒ–æŠ€å·§
        z = self.reparameterize(mu, logvar)
        
        return z, mu, logvar
    
    def reparameterize(self, mu, logvar):
        """é‡å‚æ•°åŒ–é‡‡æ ·"""
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

class MemoryModule(nn.Module):
    """M - è®°å¿†æ¨¡å—ï¼ˆæ··åˆé«˜æ–¯åˆ†å¸ƒï¼‰"""
    
    def __init__(self, memory_size, memory_dim, num_gaussians):
        super().__init__()
        self.memory_size = memory_size
        self.memory_dim = memory_dim
        self.num_gaussians = num_gaussians
        
        # è®°å¿†æ§½å‚æ•°
        self.memory_keys = nn.Parameter(torch.randn(memory_size, memory_dim))
        self.memory_values = nn.Parameter(torch.randn(memory_size, memory_dim))
        
        # æ··åˆé«˜æ–¯åˆ†å¸ƒå‚æ•°
        self.gaussian_means = nn.Parameter(torch.randn(num_gaussians, memory_dim))
        self.gaussian_logvars = nn.Parameter(torch.zeros(num_gaussians, memory_dim))
        self.gaussian_weights = nn.Parameter(torch.ones(num_gaussians))
        
        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.MultiheadAttention(
            embed_dim=memory_dim,
            num_heads=4,
            batch_first=True
        )
        
    def forward(self, query):
        """
        è®°å¿†æ£€ç´¢å’Œæ›´æ–°
        
        Args:
            query: æŸ¥è¯¢å‘é‡ [batch_size, memory_dim]
            
        Returns:
            retrieved_memory: æ£€ç´¢åˆ°çš„è®°å¿† [batch_size, memory_dim]
            attention_weights: æ³¨æ„åŠ›æƒé‡ [batch_size, memory_size]
            mixture_probs: æ··åˆæ¦‚ç‡ [batch_size, num_gaussians]
        """
        batch_size = query.size(0)
        
        # æ‰©å±•è®°å¿†é”®ä»¥åŒ¹é…æ‰¹æ¬¡å¤§å°
        keys = self.memory_keys.unsqueeze(0).expand(batch_size, -1, -1)
        values = self.memory_values.unsqueeze(0).expand(batch_size, -1, -1)
        
        # æ³¨æ„åŠ›æœºåˆ¶æ£€ç´¢è®°å¿†
        query_expanded = query.unsqueeze(1)  # [batch_size, 1, memory_dim]
        retrieved_memory, attention_weights = self.attention(
            query_expanded, keys, values
        )
        retrieved_memory = retrieved_memory.squeeze(1)  # [batch_size, memory_dim]
        attention_weights = attention_weights.squeeze(1)  # [batch_size, memory_size]
        
        # è®¡ç®—æ··åˆé«˜æ–¯åˆ†å¸ƒæ¦‚ç‡
        mixture_probs = self.compute_mixture_probabilities(query)
        
        return retrieved_memory, attention_weights, mixture_probs
    
    def compute_mixture_probabilities(self, query):
        """è®¡ç®—æ··åˆé«˜æ–¯åˆ†å¸ƒæ¦‚ç‡"""
        batch_size = query.size(0)
        
        # è®¡ç®—æ¯ä¸ªé«˜æ–¯åˆ†é‡çš„æ¦‚ç‡
        log_probs = []
        weights = F.softmax(self.gaussian_weights, dim=0)
        
        for i in range(self.num_gaussians):
            mean = self.gaussian_means[i]
            logvar = self.gaussian_logvars[i]
            
            # è®¡ç®—é«˜æ–¯æ¦‚ç‡å¯†åº¦
            diff = query - mean.unsqueeze(0)
            var = torch.exp(logvar).unsqueeze(0)
            log_prob = -0.5 * torch.sum((diff ** 2) / var + logvar.unsqueeze(0), dim=1)
            log_prob = log_prob + torch.log(weights[i])
            log_probs.append(log_prob)
        
        log_probs = torch.stack(log_probs, dim=1)  # [batch_size, num_gaussians]
        probs = F.softmax(log_probs, dim=1)
        
        return probs
    
    def update_memory(self, queries, values, learning_rate=0.01):
        """è½¯æ›´æ–°è®°å¿†"""
        with torch.no_grad():
            # è®¡ç®—æ³¨æ„åŠ›æƒé‡
            batch_size = queries.size(0)
            keys = self.memory_keys.unsqueeze(0).expand(batch_size, -1, -1)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = torch.bmm(
                queries.unsqueeze(1), 
                keys.transpose(1, 2)
            ).squeeze(1)  # [batch_size, memory_size]
            
            attention_weights = F.softmax(similarities, dim=1)
            
            # è½¯æ›´æ–°è®°å¿†å€¼
            for i in range(self.memory_size):
                weight = attention_weights[:, i].mean()
                if weight > 0.1:  # åªæ›´æ–°è¢«æ˜¾è‘—æ¿€æ´»çš„è®°å¿†æ§½
                    update = values.mean(0) * learning_rate * weight
                    self.memory_values.data[i] += update

class Controller(nn.Module):
    """C - æ§åˆ¶å™¨ç»„ä»¶"""
    
    def __init__(self, variational_dim, memory_dim, hidden_dim, output_dim):
        super().__init__()
        self.variational_dim = variational_dim
        self.memory_dim = memory_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        
        # èåˆç½‘ç»œ
        self.fusion_layer = nn.Sequential(
            nn.Linear(variational_dim + memory_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # è¾“å‡ºå±‚
        self.output_layer = nn.Linear(hidden_dim, output_dim)
        
        # é—¨æ§æœºåˆ¶
        self.gate = nn.Sequential(
            nn.Linear(variational_dim + memory_dim, 1),
            nn.Sigmoid()
        )
        
    def forward(self, variational_code, memory_code):
        """
        æ§åˆ¶å™¨å‰å‘ä¼ æ’­
        
        Args:
            variational_code: å˜åˆ†ç¼–ç  [batch_size, variational_dim]
            memory_code: è®°å¿†ç¼–ç  [batch_size, memory_dim]
            
        Returns:
            output: æœ€ç»ˆè¾“å‡º [batch_size, output_dim]
            gate_weight: é—¨æ§æƒé‡ [batch_size, 1]
        """
        # æ‹¼æ¥å˜åˆ†ç¼–ç å’Œè®°å¿†ç¼–ç 
        combined = torch.cat([variational_code, memory_code], dim=1)
        
        # é—¨æ§æœºåˆ¶
        gate_weight = self.gate(combined)
        
        # èåˆå¤„ç†
        fused = self.fusion_layer(combined)
        
        # é—¨æ§è¾“å‡º
        gated_output = fused * gate_weight
        
        # æœ€ç»ˆè¾“å‡º
        output = self.output_layer(gated_output)
        
        return output, gate_weight

class VMC(nn.Module):
    """å®Œæ•´çš„VMCæ¨¡å‹"""
    
    def __init__(self, config=None):
        super().__init__()
        if config is None:
            config = VMCConfig()
        
        self.config = config
        
        # åˆå§‹åŒ–ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶
        self.variational_encoder = VariationalEncoder(
            input_dim=config.input_dim,
            latent_dim=config.variational_dim,
            hidden_dims=config.encoder_hidden_dims
        )
        
        self.memory_module = MemoryModule(
            memory_size=config.memory_size,
            memory_dim=config.memory_dim,
            num_gaussians=config.num_gaussians
        )
        
        self.controller = Controller(
            variational_dim=config.variational_dim,
            memory_dim=config.memory_dim,
            hidden_dim=config.controller_hidden_dim,
            output_dim=config.controller_output_dim
        )
        
        # æŠ•å½±å±‚ï¼šå°†å˜åˆ†ç¼–ç æŠ•å½±åˆ°è®°å¿†ç©ºé—´
        self.var_to_memory = nn.Linear(config.variational_dim, config.memory_dim)
        
    def forward(self, x):
        """
        VMCå‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥æ•°æ® [batch_size, input_dim]
            
        Returns:
            output: åˆ†ç±»è¾“å‡º [batch_size, output_dim]
            components: å„ç»„ä»¶çš„è¾“å‡ºå’Œä¸­é—´ç»“æœ
        """
        # V - å˜åˆ†ç¼–ç 
        var_code, var_mu, var_logvar = self.variational_encoder(x)
        
        # å°†å˜åˆ†ç¼–ç æŠ•å½±åˆ°è®°å¿†ç©ºé—´
        memory_query = self.var_to_memory(var_code)
        
        # M - è®°å¿†æ£€ç´¢
        retrieved_memory, attention_weights, mixture_probs = self.memory_module(memory_query)
        
        # C - æ§åˆ¶å™¨å¤„ç†
        output, gate_weight = self.controller(var_code, retrieved_memory)
        
        # è¿”å›æ‰€æœ‰ç»„ä»¶çš„ç»“æœç”¨äºå¯è§†åŒ–
        components = {
            'variational': {
                'code': var_code,
                'mu': var_mu,
                'logvar': var_logvar
            },
            'memory': {
                'retrieved': retrieved_memory,
                'attention_weights': attention_weights,
                'mixture_probs': mixture_probs,
                'query': memory_query
            },
            'controller': {
                'output': output,
                'gate_weight': gate_weight
            }
        }
        
        return output, components
    
    def compute_loss(self, x, y, output, components):
        """è®¡ç®—VMCæ€»æŸå¤±"""
        # åˆ†ç±»æŸå¤±
        classification_loss = F.cross_entropy(output, y)
        
        # KLæ•£åº¦æŸå¤±ï¼ˆå˜åˆ†éƒ¨åˆ†ï¼‰
        var_mu = components['variational']['mu']
        var_logvar = components['variational']['logvar']
        kl_loss = -0.5 * torch.sum(1 + var_logvar - var_mu.pow(2) - var_logvar.exp())
        kl_loss = kl_loss / x.size(0)  # å¹³å‡åˆ°batch
        
        # è®°å¿†æ­£åˆ™åŒ–æŸå¤±
        memory_loss = self.compute_memory_regularization()
        
        # æ€»æŸå¤±
        total_loss = (classification_loss + 
                     self.config.kl_beta * kl_loss + 
                     self.config.memory_beta * memory_loss)
        
        return {
            'total_loss': total_loss,
            'classification_loss': classification_loss,
            'kl_loss': kl_loss,
            'memory_loss': memory_loss
        }
    
    def compute_memory_regularization(self):
        """è®¡ç®—è®°å¿†æ­£åˆ™åŒ–æŸå¤±"""
        # è®°å¿†æ§½å¤šæ ·æ€§æŸå¤±
        memory_keys = self.memory_module.memory_keys
        memory_values = self.memory_module.memory_values
        
        # è®¡ç®—è®°å¿†é”®ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œé¼“åŠ±å¤šæ ·æ€§
        key_sim = torch.mm(memory_keys, memory_keys.t())
        key_sim = key_sim - torch.eye(key_sim.size(0)).to(key_sim.device)
        diversity_loss = torch.sum(key_sim ** 2)
        
        # æ··åˆé«˜æ–¯åˆ†å¸ƒçš„æ­£åˆ™åŒ–
        gaussian_weights = F.softmax(self.memory_module.gaussian_weights, dim=0)
        entropy_loss = -torch.sum(gaussian_weights * torch.log(gaussian_weights + 1e-8))
        
        return diversity_loss * 0.01 - entropy_loss * 0.1

def create_vmc_model(config=None):
    """åˆ›å»ºVMCæ¨¡å‹çš„å·¥å‚å‡½æ•°"""
    if config is None:
        config = VMCConfig()
    
    model = VMC(config).to(config.device)
    
    # ç»Ÿè®¡å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"ğŸ—ï¸  VMCæ¨¡å‹åˆ›å»ºå®Œæˆ")
    print(f"   æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"   å˜åˆ†ç¼–ç ç»´åº¦: {config.variational_dim}")
    print(f"   è®°å¿†æ§½æ•°é‡: {config.memory_size}")
    print(f"   è®°å¿†ç»´åº¦: {config.memory_dim}")
    print(f"   æ··åˆé«˜æ–¯ç»„ä»¶æ•°: {config.num_gaussians}")
    
    return model

def test_vmc_model():
    """æµ‹è¯•VMCæ¨¡å‹"""
    print("ğŸ§ª æµ‹è¯•VMCæ¨¡å‹...")
    
    config = VMCConfig()
    model = create_vmc_model(config)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size = 4
    x = torch.randn(batch_size, config.input_dim).to(config.device)
    y = torch.randint(0, config.controller_output_dim, (batch_size,)).to(config.device)
    
    # å‰å‘ä¼ æ’­
    output, components = model(x)
    
    # è®¡ç®—æŸå¤±
    losses = model.compute_loss(x, y, output, components)
    
    print(f"âœ… æ¨¡å‹æµ‹è¯•é€šè¿‡")
    print(f"   è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"   æ€»æŸå¤±: {losses['total_loss'].item():.4f}")
    print(f"   åˆ†ç±»æŸå¤±: {losses['classification_loss'].item():.4f}")
    print(f"   KLæŸå¤±: {losses['kl_loss'].item():.4f}")
    print(f"   è®°å¿†æŸå¤±: {losses['memory_loss'].item():.4f}")

if __name__ == "__main__":
    test_vmc_model()
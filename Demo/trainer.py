"""
VMCè®­ç»ƒæ¨¡å— - æ•´åˆVã€Mã€Cä¸‰ä¸ªç»„ä»¶çš„è®­ç»ƒæµç¨‹
"""
import torch
import torch.optim as optim
import numpy as np
import os
from datetime import datetime

from config import VMCConfig
from vmc_model import create_vmc_model
from dataset import VMCDataset
from visualizer import VMCVisualizer

class VMCTrainer:
    """VMCè®­ç»ƒå™¨"""
    
    def __init__(self, config=None):
        self.config = config if config else VMCConfig()
        
        # åˆ›å»ºæ•°æ®é›†
        self.dataset_manager = VMCDataset()
        self.train_loader, self.test_loader = self.dataset_manager.create_dataloaders()
        
        # åˆ›å»ºæ¨¡å‹
        self.model = create_vmc_model(self.config)
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.config.learning_rate)
        
        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=self.config.epochs, eta_min=1e-6
        )
        
        # åˆ›å»ºå¯è§†åŒ–å™¨
        self.visualizer = VMCVisualizer(self.model, self.config)
        
        # è®­ç»ƒç»Ÿè®¡
        self.train_losses = []
        self.train_accuracies = []
        self.test_accuracies = []
        self.component_stats = {
            'variational': [],
            'memory': [],
            'controller': []
        }
        
        print(f"ğŸš€ VMCè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   æ¨¡å‹å‚æ•°: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"   è®­ç»ƒæ•°æ®: {len(self.train_loader.dataset)}")
        print(f"   æµ‹è¯•æ•°æ®: {len(self.test_loader.dataset)}")
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        total_loss = 0
        total_classification_loss = 0
        total_kl_loss = 0
        total_memory_loss = 0
        correct_predictions = 0
        total_samples = 0
        
        for batch_idx, (x, y) in enumerate(self.train_loader):
            # æ•°æ®é¢„å¤„ç†
            x = x.view(x.size(0), -1).to(self.config.device)
            y = y.to(self.config.device)
            
            # å‰å‘ä¼ æ’­
            output, components = self.model(x)
            
            # è®¡ç®—æŸå¤±
            losses = self.model.compute_loss(x, y, output, components)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            losses['total_loss'].backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += losses['total_loss'].item()
            total_classification_loss += losses['classification_loss'].item()
            total_kl_loss += losses['kl_loss'].item()
            total_memory_loss += losses['memory_loss'].item()
            
            # è®¡ç®—å‡†ç¡®ç‡
            predicted = torch.argmax(output, dim=1)
            correct_predictions += (predicted == y).sum().item()
            total_samples += y.size(0)
            
            # å®šæœŸæ›´æ–°è®°å¿†
            if batch_idx % 10 == 0:
                memory_query = self.model.var_to_memory(components['variational']['code'])
                self.model.memory_module.update_memory(
                    memory_query, components['memory']['retrieved'], learning_rate=0.01
                )
            
            # æ‰“å°è®­ç»ƒè¿›åº¦
            if batch_idx % 100 == 0:
                current_lr = self.scheduler.get_last_lr()[0]
                print(f'   Epoch {epoch+1}/{self.config.epochs}, '
                      f'Batch {batch_idx}/{len(self.train_loader)}, '
                      f'Loss: {losses["total_loss"].item():.4f}, '
                      f'Acc: {100.*correct_predictions/total_samples:.2f}%, '
                      f'LR: {current_lr:.2e}')
        
        # è®¡ç®—å¹³å‡å€¼
        avg_loss = total_loss / len(self.train_loader)
        avg_classification_loss = total_classification_loss / len(self.train_loader)
        avg_kl_loss = total_kl_loss / len(self.train_loader)
        avg_memory_loss = total_memory_loss / len(self.train_loader)
        train_accuracy = 100. * correct_predictions / total_samples
        
        return {
            'avg_loss': avg_loss,
            'classification_loss': avg_classification_loss,
            'kl_loss': avg_kl_loss,
            'memory_loss': avg_memory_loss,
            'train_accuracy': train_accuracy
        }
    
    def evaluate(self, epoch):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        self.model.eval()
        
        total_loss = 0
        correct_predictions = 0
        total_samples = 0
        
        with torch.no_grad():
            for x, y in self.test_loader:
                x = x.view(x.size(0), -1).to(self.config.device)
                y = y.to(self.config.device)
                
                # å‰å‘ä¼ æ’­
                output, components = self.model(x)
                
                # è®¡ç®—æŸå¤±
                losses = self.model.compute_loss(x, y, output, components)
                total_loss += losses['total_loss'].item()
                
                # è®¡ç®—å‡†ç¡®ç‡
                predicted = torch.argmax(output, dim=1)
                correct_predictions += (predicted == y).sum().item()
                total_samples += y.size(0)
        
        test_accuracy = 100. * correct_predictions / total_samples
        avg_test_loss = total_loss / len(self.test_loader)
        
        return {
            'test_accuracy': test_accuracy,
            'test_loss': avg_test_loss
        }
    
    def visualize_components(self, epoch, save_dir="results", skip_visualization=False):
        """å¯è§†åŒ–å„ä¸ªç»„ä»¶"""
        if skip_visualization:
            print(f"ğŸ¨ è·³è¿‡ç»„ä»¶å¯è§†åŒ– - Epoch {epoch} (æ¼”ç¤ºæ¨¡å¼)")
            return
            
        print(f"ğŸ¨ å¼€å§‹å¯è§†åŒ–å„ç»„ä»¶ - Epoch {epoch}")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        
        try:
            # å¯è§†åŒ–Vç»„ä»¶
            print("   ğŸ“Š å¯è§†åŒ–Vç»„ä»¶...")
            v_stats = self.visualizer.visualize_variational_component(
                self.test_loader, epoch, save_dir
            )
            self.component_stats['variational'].append(v_stats)
            
            # å¯è§†åŒ–Mç»„ä»¶  
            print("   ğŸ§  å¯è§†åŒ–Mç»„ä»¶...")
            m_stats = self.visualizer.visualize_memory_component(
                self.test_loader, epoch, save_dir
            )
            self.component_stats['memory'].append(m_stats)
            
            # å¯è§†åŒ–Cç»„ä»¶
            print("   ğŸ® å¯è§†åŒ–Cç»„ä»¶...")
            c_stats = self.visualizer.visualize_controller_component(
                self.test_loader, epoch, save_dir
            )
            self.component_stats['controller'].append(c_stats)
            
            # å¯è§†åŒ–å®Œæ•´æµæ°´çº¿
            print("   ğŸ”„ å¯è§†åŒ–å®Œæ•´æµæ°´çº¿...")
            pipeline_path = self.visualizer.visualize_complete_pipeline(
                self.test_loader, epoch, save_dir
            )
            
            print(f"âœ… æ‰€æœ‰ç»„ä»¶å¯è§†åŒ–å®Œæˆ - Epoch {epoch}")
            
            return {
                'variational_stats': v_stats,
                'memory_stats': m_stats,
                'controller_stats': c_stats,
                'pipeline_path': pipeline_path
            }
        except Exception as e:
            print(f"   âŒ å¯è§†åŒ–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return {
                'variational_stats': None,
                'memory_stats': None,
                'controller_stats': None,
                'pipeline_path': None
            }
    
    def save_checkpoint(self, epoch, save_dir="checkpoints"):
        """ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹"""
        os.makedirs(save_dir, exist_ok=True)
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'train_losses': self.train_losses,
            'train_accuracies': self.train_accuracies,
            'test_accuracies': self.test_accuracies,
            'component_stats': self.component_stats,
            'config': self.config
        }
        
        checkpoint_path = os.path.join(save_dir, f'vmc_checkpoint_epoch_{epoch}.pth')
        torch.save(checkpoint, checkpoint_path)
        
        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
        return checkpoint_path
    
    def load_checkpoint(self, checkpoint_path):
        """åŠ è½½è®­ç»ƒæ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(checkpoint_path, map_location=self.config.device, weights_only=False)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        self.train_losses = checkpoint.get('train_losses', [])
        self.train_accuracies = checkpoint.get('train_accuracies', [])
        self.test_accuracies = checkpoint.get('test_accuracies', [])
        self.component_stats = checkpoint.get('component_stats', {
            'variational': [], 'memory': [], 'controller': []
        })
        
        start_epoch = checkpoint['epoch'] + 1
        
        print(f"ğŸ“‚ æ£€æŸ¥ç‚¹å·²åŠ è½½: {checkpoint_path}")
        print(f"   ç»§ç»­ä»epoch {start_epoch}å¼€å§‹è®­ç»ƒ")
        
        return start_epoch
    
    def train(self, start_epoch=0, demo_mode=False):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"ğŸ¯ å¼€å§‹VMCè®­ç»ƒ")
        print(f"   è®­ç»ƒè½®æ•°: {self.config.epochs}")
        print(f"   å¼€å§‹epoch: {start_epoch}")
        print("=" * 80)
        
        start_time = datetime.now()
        
        try:
            for epoch in range(start_epoch, self.config.epochs):
                print(f"\nğŸ“ˆ Epoch {epoch+1}/{self.config.epochs}")
                print("-" * 60)
                
                # è®­ç»ƒä¸€ä¸ªepoch
                train_stats = self.train_epoch(epoch)
                
                # è¯„ä¼°æ¨¡å‹
                eval_stats = self.evaluate(epoch)
                
                # æ›´æ–°å­¦ä¹ ç‡
                self.scheduler.step()
                
                # è®°å½•ç»Ÿè®¡æ•°æ®
                self.train_losses.append(train_stats['avg_loss'])
                self.train_accuracies.append(train_stats['train_accuracy'])
                self.test_accuracies.append(eval_stats['test_accuracy'])
                
                # æ‰“å°epochæ€»ç»“
                print(f"\nâœ… Epoch {epoch+1} å®Œæˆ:")
                print(f"   è®­ç»ƒæŸå¤±: {train_stats['avg_loss']:.4f}")
                print(f"   åˆ†ç±»æŸå¤±: {train_stats['classification_loss']:.4f}")
                print(f"   KLæŸå¤±: {train_stats['kl_loss']:.4f}")
                print(f"   è®°å¿†æŸå¤±: {train_stats['memory_loss']:.4f}")
                print(f"   è®­ç»ƒå‡†ç¡®ç‡: {train_stats['train_accuracy']:.2f}%")
                print(f"   æµ‹è¯•å‡†ç¡®ç‡: {eval_stats['test_accuracy']:.2f}%")
                
                # å®šæœŸå¯è§†åŒ–ç»„ä»¶
                if (epoch + 1) % self.config.save_interval == 0:
                    vis_stats = self.visualize_components(epoch + 1, skip_visualization=demo_mode)
                    
                    # æ‰“å°ç»„ä»¶ç»Ÿè®¡
                    if vis_stats and not demo_mode:
                        print(f"\nğŸ” ç»„ä»¶åˆ†æ - Epoch {epoch+1}:")
                        if vis_stats['variational_stats']:
                            print(f"   Vç»„ä»¶ - å¹³å‡KLæ•£åº¦: {vis_stats['variational_stats']['mean_kl_div']:.4f}")
                        if vis_stats['memory_stats']:
                            print(f"   Mç»„ä»¶ - æ³¨æ„åŠ›ç†µ: {vis_stats['memory_stats']['mean_attention_entropy']:.4f}")
                        if vis_stats['controller_stats']:
                            print(f"   Cç»„ä»¶ - åˆ†ç±»å‡†ç¡®ç‡: {vis_stats['controller_stats']['accuracy']:.4f}")
                
                # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                if (epoch + 1) % (self.config.save_interval * 2) == 0:
                    self.save_checkpoint(epoch + 1)
                
                print("-" * 60)
        
        except KeyboardInterrupt:
            print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
            self.save_checkpoint(epoch + 1)
        
        except Exception as e:
            print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            self.save_checkpoint(epoch + 1)
            raise
        
        finally:
            end_time = datetime.now()
            training_time = end_time - start_time
            
            print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
            print(f"   æ€»è®­ç»ƒæ—¶é—´: {training_time}")
            print(f"   æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {self.train_accuracies[-1]:.2f}%")
            print(f"   æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {self.test_accuracies[-1]:.2f}%")
            
            # æœ€ç»ˆå¯è§†åŒ–
            if not demo_mode:
                print(f"\nğŸ¨ ç”Ÿæˆæœ€ç»ˆå¯è§†åŒ–ç»“æœ...")
                final_vis = self.visualize_components(self.config.epochs)
            else:
                print(f"\nğŸ¨ è·³è¿‡å†…ç½®å¯è§†åŒ– (æ¼”ç¤ºæ¨¡å¼)")
                final_vis = None
            
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            final_checkpoint = self.save_checkpoint(self.config.epochs)
            
            return {
                'model': self.model,
                'training_time': training_time,
                'final_train_accuracy': self.train_accuracies[-1],
                'final_test_accuracy': self.test_accuracies[-1],
                'checkpoint_path': final_checkpoint,
                'visualization_stats': final_vis
            }

def quick_train():
    """å¿«é€Ÿè®­ç»ƒå‡½æ•°"""
    print("ğŸš€ å¯åŠ¨VMCå¿«é€Ÿè®­ç»ƒ...")
    
    # æ˜¾ç¤ºé…ç½®
    VMCConfig.print_config()
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = VMCTrainer()
        
        # å¼€å§‹è®­ç»ƒ
        results = trainer.train()
        
        print("âœ… å¿«é€Ÿè®­ç»ƒæˆåŠŸå®Œæˆ!")
        return results
        
    except Exception as e:
        print(f"âŒ å¿«é€Ÿè®­ç»ƒå¤±è´¥: {e}")
        return None

if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œè®­ç»ƒ
    results = quick_train()
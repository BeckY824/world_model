#!/usr/bin/env python3
"""
æœ€ç»ˆçš„VMCå¯è§†åŒ–è„šæœ¬ - æŒ‰ç”¨æˆ·éœ€æ±‚å®šåˆ¶
1. Vç»„ä»¶: æ½œåœ¨ç¼–ç è´¨é‡è¯„ä¼°æŒ‡æ ‡
2. Mç»„ä»¶: æ¦‚ç‡åˆ†å¸ƒå¯è§†åŒ–
3. ç§»é™¤Cç»„ä»¶å¯è§†åŒ–
"""
import matplotlib
matplotlib.use('Agg')  # å¼ºåˆ¶ä½¿ç”¨éäº¤äº’å¼åç«¯

import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import os
from config import VMCConfig
from vmc_model import create_vmc_model
from dataset import VMCDataset

# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans']
plt.rcParams['axes.unicode_minus'] = False

def final_visualize_components():
    """æœ€ç»ˆçš„VMCç»„ä»¶å¯è§†åŒ– - ç¬¦åˆç”¨æˆ·éœ€æ±‚"""
    
    print("ğŸ¨ æœ€ç»ˆVMCå¯è§†åŒ–å¼€å§‹...")
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs('results', exist_ok=True)
    
    try:
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint_path = "checkpoints/vmc_checkpoint_epoch_3.pth"
        print(f"ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=VMCConfig.device, weights_only=False)
        
        # åˆ›å»ºæ¨¡å‹å¹¶åŠ è½½æƒé‡
        model = create_vmc_model(VMCConfig())
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # åŠ è½½æ•°æ®
        dataset = VMCDataset()
        train_loader, test_loader = dataset.create_dataloaders()
        
        # æ”¶é›†æ›´å¤šæ•°æ®ç”¨äºç»Ÿè®¡åˆ†æ
        all_v_outputs = []
        all_mu = []
        all_logvar = []
        all_attention_weights = []
        all_mixture_probs = []
        all_labels = []
        
        print("ğŸ“Š æ”¶é›†æ•°æ®è¿›è¡Œåˆ†æ...")
        with torch.no_grad():
            for batch_idx, (images, labels) in enumerate(test_loader):
                if batch_idx >= 10:  # åªå¤„ç†å‰10ä¸ªæ‰¹æ¬¡
                    break
                    
                images = images.to(VMCConfig.device)
                labels = labels.to(VMCConfig.device)
                
                # æ‰å¹³åŒ–å›¾åƒ [batch_size, 1, 28, 28] -> [batch_size, 784]
                images_flat = images.view(images.size(0), -1)
                
                # Vç»„ä»¶ - å˜åˆ†ç¼–ç å™¨
                v_output, mu, logvar = model.variational_encoder(images_flat)
                
                # å°†å˜åˆ†ç¼–ç è½¬æ¢ä¸ºè®°å¿†æŸ¥è¯¢ (64 -> 32ç»´)
                memory_query = model.var_to_memory(v_output)
                
                # Mç»„ä»¶ - è®°å¿†æ¨¡å—  
                memory_output, attention_weights, mixture_probs = model.memory_module(memory_query)
                
                # æ”¶é›†æ•°æ®
                all_v_outputs.append(v_output.cpu())
                all_mu.append(mu.cpu())
                all_logvar.append(logvar.cpu())
                all_attention_weights.append(attention_weights.cpu())
                all_mixture_probs.append(mixture_probs.cpu())
                all_labels.append(labels.cpu())
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        v_data = torch.cat(all_v_outputs, dim=0).numpy()
        mu_data = torch.cat(all_mu, dim=0).numpy()
        logvar_data = torch.cat(all_logvar, dim=0).numpy()
        attention_data = torch.cat(all_attention_weights, dim=0).numpy()
        mixture_data = torch.cat(all_mixture_probs, dim=0).numpy()
        labels_data = torch.cat(all_labels, dim=0).numpy()
        
        print("ğŸ“Š å¼€å§‹ç”Ÿæˆæœ€ç»ˆå¯è§†åŒ–å›¾è¡¨...")
        
        # ==================== 1. Vç»„ä»¶è¯„ä¼°æŒ‡æ ‡ ====================
        print("   ç”ŸæˆVç»„ä»¶è¯„ä¼°æŒ‡æ ‡...")
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1.1 æ½œåœ¨ç©ºé—´åˆ†å¸ƒ (PCA 2D)
        pca = PCA(n_components=2)
        v_2d = pca.fit_transform(v_data)
        scatter = axes[0, 0].scatter(v_2d[:, 0], v_2d[:, 1], c=labels_data, cmap='tab10', alpha=0.6, s=20)
        axes[0, 0].set_title('V Component: Latent Space Distribution (PCA)')
        axes[0, 0].set_xlabel(f'PC1 (Explained Variance: {pca.explained_variance_ratio_[0]:.3f})')
        axes[0, 0].set_ylabel(f'PC2 (Explained Variance: {pca.explained_variance_ratio_[1]:.3f})')
        plt.colorbar(scatter, ax=axes[0, 0])
        
        # 1.2 KLæ•£åº¦åˆ†å¸ƒ
        kl_divergence = -0.5 * torch.sum(1 + torch.tensor(logvar_data) - torch.tensor(mu_data).pow(2) - torch.tensor(logvar_data).exp(), dim=1)
        axes[0, 1].hist(kl_divergence.numpy(), bins=30, alpha=0.7, color='red', edgecolor='black')
        axes[0, 1].set_title('V Component: KL Divergence Distribution')
        axes[0, 1].set_xlabel('KL Divergence')
        axes[0, 1].set_ylabel('Frequency')
        axes[0, 1].axvline(np.mean(kl_divergence.numpy()), color='darkred', linestyle='--', linewidth=2,
                          label=f'Mean: {np.mean(kl_divergence.numpy()):.3f}')
        axes[0, 1].legend()
        
        # 1.3 æ½œåœ¨ç»´åº¦æ¿€æ´»åº¦
        latent_activation = np.mean(np.abs(v_data), axis=0)
        axes[0, 2].bar(range(len(latent_activation)), latent_activation, alpha=0.7, color='blue')
        axes[0, 2].set_title('V Component: Latent Dimension Activation')
        axes[0, 2].set_xlabel('Latent Dimension')
        axes[0, 2].set_ylabel('Average Absolute Activation')
        axes[0, 2].set_xticks(range(0, len(latent_activation), 8))
        
        # 1.4 ç±»åˆ«åˆ†ç¦»åº¦è¯„ä¼°
        class_centers = []
        for class_id in range(10):
            mask = labels_data == class_id
            if np.sum(mask) > 0:
                center = np.mean(v_data[mask], axis=0)
                class_centers.append(center)
        
        if len(class_centers) > 1:
            class_centers = np.array(class_centers)
            # è®¡ç®—ç±»åˆ«ä¸­å¿ƒä¹‹é—´çš„è·ç¦»
            from scipy.spatial.distance import pdist
            distances = pdist(class_centers)
            axes[1, 0].hist(distances, bins=20, alpha=0.7, color='green', edgecolor='black')
            axes[1, 0].set_title('V Component: Inter-class Distances')
            axes[1, 0].set_xlabel('Distance between Class Centers')
            axes[1, 0].set_ylabel('Frequency')
            axes[1, 0].axvline(np.mean(distances), color='darkgreen', linestyle='--', linewidth=2,
                              label=f'Mean: {np.mean(distances):.3f}')
            axes[1, 0].legend()
        
        # 1.5 æ–¹å·®åˆ†æ
        variances = np.exp(logvar_data)  # è½¬æ¢logæ–¹å·®ä¸ºæ–¹å·®
        mean_variance_per_dim = np.mean(variances, axis=0)
        axes[1, 1].plot(mean_variance_per_dim, marker='o', markersize=4, alpha=0.7)
        axes[1, 1].set_title('V Component: Average Variance per Dimension')
        axes[1, 1].set_xlabel('Latent Dimension')
        axes[1, 1].set_ylabel('Average Variance')
        axes[1, 1].grid(True, alpha=0.3)
        
        # 1.6 ç¼–ç è´¨é‡è¯„åˆ†
        # åŸºäºKLæ•£åº¦ã€ç±»åˆ«åˆ†ç¦»åº¦ç­‰è®¡ç®—ç»¼åˆè¯„åˆ†
        avg_kl = np.mean(kl_divergence.numpy())
        avg_separation = np.mean(distances) if len(class_centers) > 1 else 0
        total_variance = np.sum(mean_variance_per_dim)
        
        quality_metrics = ['KL Divergence', 'Class Separation', 'Total Variance', 'Latent Utilization']
        quality_scores = [
            min(avg_kl / 10.0, 1.0),  # å½’ä¸€åŒ–KLæ•£åº¦
            min(avg_separation / 5.0, 1.0),  # å½’ä¸€åŒ–åˆ†ç¦»åº¦
            min(total_variance / 50.0, 1.0),  # å½’ä¸€åŒ–æ€»æ–¹å·®
            np.mean(latent_activation > 0.1)  # æ¿€æ´»ç»´åº¦æ¯”ä¾‹
        ]
        
        bars = axes[1, 2].bar(range(len(quality_metrics)), quality_scores, 
                             color=['red', 'green', 'blue', 'orange'], alpha=0.7)
        axes[1, 2].set_title('V Component: Quality Metrics')
        axes[1, 2].set_ylabel('Normalized Score')
        axes[1, 2].set_ylim(0, 1)
        axes[1, 2].set_xticks(range(len(quality_metrics)))
        axes[1, 2].set_xticklabels(quality_metrics, rotation=45, ha='right')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, score in zip(bars, quality_scores):
            height = bar.get_height()
            axes[1, 2].text(bar.get_x() + bar.get_width()/2., height + 0.02,
                           f'{score:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig('results/v_component_evaluation_metrics.png', dpi=150, bbox_inches='tight')
        plt.close()
        print("âœ… Vç»„ä»¶è¯„ä¼°æŒ‡æ ‡å®Œæˆ: results/v_component_evaluation_metrics.png")
        
        # ==================== 2. Mç»„ä»¶æ¦‚ç‡åˆ†å¸ƒ ====================
        print("   ç”ŸæˆMç»„ä»¶æ¦‚ç‡åˆ†å¸ƒ...")
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 2.1 æ··åˆé«˜æ–¯åˆ†é‡æ¦‚ç‡åˆ†å¸ƒ
        avg_mixture_probs = np.mean(mixture_data, axis=0)
        std_mixture_probs = np.std(mixture_data, axis=0)
        
        x_pos = np.arange(len(avg_mixture_probs))
        bars = axes[0, 0].bar(x_pos, avg_mixture_probs, yerr=std_mixture_probs, 
                             alpha=0.7, color='orange', capsize=5, edgecolor='black')
        axes[0, 0].set_title('M Component: Gaussian Mixture Probabilities')
        axes[0, 0].set_xlabel('Gaussian Component ID')
        axes[0, 0].set_ylabel('Probability')
        axes[0, 0].set_xticks(x_pos)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, prob, std in zip(bars, avg_mixture_probs, std_mixture_probs):
            height = bar.get_height()
            axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + std + 0.01,
                           f'{prob:.3f}', ha='center', va='bottom', fontsize=9)
        
        # 2.2 æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒ
        avg_attention = np.mean(attention_data, axis=0)
        std_attention = np.std(attention_data, axis=0)
        
        x_pos = np.arange(len(avg_attention))
        axes[0, 1].bar(x_pos, avg_attention, yerr=std_attention, 
                      alpha=0.7, color='cyan', capsize=3, edgecolor='black')
        axes[0, 1].set_title('M Component: Memory Attention Weights')
        axes[0, 1].set_xlabel('Memory Slot ID')
        axes[0, 1].set_ylabel('Attention Weight')
        axes[0, 1].set_xticks(range(0, len(avg_attention), 2))
        
        # 2.3 æ³¨æ„åŠ›ç†µåˆ†å¸ƒï¼ˆè¡¡é‡æ³¨æ„åŠ›é›†ä¸­ç¨‹åº¦ï¼‰
        attention_entropy = []
        for i in range(len(attention_data)):
            attention = attention_data[i]
            attention = attention / (np.sum(attention) + 1e-8)  # å½’ä¸€åŒ–
            entropy = -np.sum(attention * np.log(attention + 1e-8))
            attention_entropy.append(entropy)
        
        axes[1, 0].hist(attention_entropy, bins=25, alpha=0.7, color='magenta', edgecolor='black')
        axes[1, 0].set_title('M Component: Attention Entropy Distribution')
        axes[1, 0].set_xlabel('Entropy (Higher = More Distributed)')
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].axvline(np.mean(attention_entropy), color='darkmagenta', linestyle='--', linewidth=2,
                          label=f'Mean: {np.mean(attention_entropy):.3f}')
        axes[1, 0].legend()
        
        # 2.4 æ··åˆæ¦‚ç‡çš„æ ·æœ¬é—´å˜åŒ–
        mixture_variance = np.var(mixture_data, axis=0)
        axes[1, 1].bar(range(len(mixture_variance)), mixture_variance, 
                      alpha=0.7, color='brown', edgecolor='black')
        axes[1, 1].set_title('M Component: Mixture Probability Variance')
        axes[1, 1].set_xlabel('Gaussian Component ID')
        axes[1, 1].set_ylabel('Variance Across Samples')
        axes[1, 1].set_xticks(range(len(mixture_variance)))
        
        plt.tight_layout()
        plt.savefig('results/m_component_probability_analysis.png', dpi=150, bbox_inches='tight')
        plt.close()
        print("âœ… Mç»„ä»¶æ¦‚ç‡åˆ†å¸ƒå®Œæˆ: results/m_component_probability_analysis.png")
        
        # ==================== 3. ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š ====================
        print("   ç”Ÿæˆæœ€ç»ˆè¯„ä¼°æŠ¥å‘Š...")
        
        # è®¡ç®—å…³é”®æŒ‡æ ‡
        avg_kl = np.mean(kl_divergence.numpy())
        avg_separation = np.mean(distances) if len(class_centers) > 1 else 0
        dominant_gaussian = np.argmax(avg_mixture_probs)
        dominant_prob = np.max(avg_mixture_probs)
        avg_attention_entropy = np.mean(attention_entropy)
        most_attended_slot = np.argmax(avg_attention)
        max_attention = np.max(avg_attention)
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report = f"""
æœ€ç»ˆVMC (Variational Memory Compression) è¯„ä¼°æŠ¥å‘Š
==============================================

è®­ç»ƒåŸºæœ¬ä¿¡æ¯:
- æ£€æŸ¥ç‚¹: {checkpoint_path}
- åˆ†ææ ·æœ¬æ•°: {len(v_data)}
- æ¨¡å‹å‚æ•°é‡: 620,819

Vç»„ä»¶ (å˜åˆ†ç¼–ç å™¨) æ€§èƒ½è¯„ä¼°:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
æ½œåœ¨ç©ºé—´è´¨é‡:
  â€¢ å¹³å‡KLæ•£åº¦: {avg_kl:.4f}
  â€¢ ç±»åˆ«åˆ†ç¦»åº¦: {avg_separation:.4f}
  â€¢ æ€»ä½“æ–¹å·®: {total_variance:.4f}
  â€¢ æ¿€æ´»ç»´åº¦æ¯”ä¾‹: {np.mean(latent_activation > 0.1):.3f}

ç¼–ç è´¨é‡è¯„çº§:
  â€¢ KLæ•£åº¦æ§åˆ¶: {'ä¼˜ç§€' if avg_kl < 5.0 else 'è‰¯å¥½' if avg_kl < 10.0 else 'éœ€è¦æ”¹è¿›'}
  â€¢ ç±»åˆ«åˆ†ç¦»åº¦: {'ä¼˜ç§€' if avg_separation > 3.0 else 'è‰¯å¥½' if avg_separation > 1.0 else 'éœ€è¦æ”¹è¿›'}
  â€¢ ç»´åº¦åˆ©ç”¨ç‡: {'é«˜æ•ˆ' if np.mean(latent_activation > 0.1) > 0.8 else 'ä¸­ç­‰' if np.mean(latent_activation > 0.1) > 0.5 else 'ä½æ•ˆ'}

Mç»„ä»¶ (è®°å¿†æ¨¡å—) æ¦‚ç‡åˆ†å¸ƒåˆ†æ:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
æ··åˆé«˜æ–¯åˆ†å¸ƒ:
  â€¢ ä¸»å¯¼åˆ†é‡: Component {dominant_gaussian} (æ¦‚ç‡: {dominant_prob:.3f})
  â€¢ åˆ†å¸ƒå‡åŒ€æ€§: {'å‡åŒ€' if dominant_prob < 0.3 else 'ä¸­ç­‰' if dominant_prob < 0.5 else 'é›†ä¸­'}
  â€¢ ç»„ä»¶æ•°é‡: {len(avg_mixture_probs)}

è®°å¿†æ³¨æ„åŠ›æœºåˆ¶:
  â€¢ æœ€å—å…³æ³¨è®°å¿†æ§½: Slot {most_attended_slot} (æƒé‡: {max_attention:.3f})
  â€¢ å¹³å‡æ³¨æ„åŠ›ç†µ: {avg_attention_entropy:.3f}
  â€¢ æ³¨æ„åŠ›åˆ†å¸ƒ: {'åˆ†æ•£' if avg_attention_entropy > 2.5 else 'é€‚ä¸­' if avg_attention_entropy > 1.5 else 'é›†ä¸­'}

æ•´ä½“æ€§èƒ½è¯„ä¼°:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Vç»„ä»¶è¡¨ç°: {'ä¼˜ç§€' if avg_kl < 5.0 and avg_separation > 2.0 else 'è‰¯å¥½' if avg_kl < 8.0 else 'å¯æ¥å—'}
Mç»„ä»¶è¡¨ç°: {'æœ‰æ•ˆ' if avg_attention_entropy < 3.0 and dominant_prob < 0.6 else 'å¯æ¥å—'}

å»ºè®®ä¼˜åŒ–æ–¹å‘:
{f'â€¢ å‡å°‘KLæ•£åº¦ä»¥æé«˜ç¼–ç æ•ˆç‡' if avg_kl > 8.0 else 'â€¢ KLæ•£åº¦æ§åˆ¶è‰¯å¥½'}
{f'â€¢ æé«˜ç±»åˆ«åˆ†ç¦»åº¦' if avg_separation < 2.0 else 'â€¢ ç±»åˆ«åˆ†ç¦»åº¦è‰¯å¥½'}
{f'â€¢ å¹³è¡¡æ··åˆåˆ†é‡åˆ†å¸ƒ' if dominant_prob > 0.5 else 'â€¢ æ··åˆåˆ†é‡åˆ†å¸ƒåˆç†'}
{f'â€¢ è°ƒæ•´æ³¨æ„åŠ›æœºåˆ¶é›†ä¸­åº¦' if avg_attention_entropy > 3.0 else 'â€¢ æ³¨æ„åŠ›æœºåˆ¶å·¥ä½œæ­£å¸¸'}

ç”Ÿæˆçš„å¯è§†åŒ–æ–‡ä»¶:
1. results/v_component_evaluation_metrics.png - Vç»„ä»¶å…¨é¢è¯„ä¼°
2. results/m_component_probability_analysis.png - Mç»„ä»¶æ¦‚ç‡åˆ†æ

ç»“è®º: VMCæ¨¡å‹åœ¨å˜åˆ†ç¼–ç å’Œè®°å¿†æœºåˆ¶æ–¹é¢å±•ç°äº†{'è‰¯å¥½' if avg_kl < 8.0 and avg_attention_entropy < 3.5 else 'å¯æ¥å—'}çš„æ€§èƒ½è¡¨ç°ã€‚
"""
        
        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        with open('results/final_vmc_evaluation_report.txt', 'w', encoding='utf-8') as f:
            f.write(report)
        
        print("\nğŸ‰ æœ€ç»ˆVMCå¯è§†åŒ–å…¨éƒ¨å®Œæˆ!")
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print("ğŸ“Š ç”Ÿæˆçš„å¯è§†åŒ–æ–‡ä»¶:")
        print("   â€¢ results/v_component_evaluation_metrics.png - Vç»„ä»¶æ€§èƒ½è¯„ä¼°")
        print("   â€¢ results/m_component_probability_analysis.png - Mç»„ä»¶æ¦‚ç‡åˆ†å¸ƒ")
        print("   â€¢ results/final_vmc_evaluation_report.txt - æœ€ç»ˆè¯„ä¼°æŠ¥å‘Š")
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print(f"ğŸ“ˆ å…³é”®æ€§èƒ½æŒ‡æ ‡:")
        print(f"   â€¢ Vç»„ä»¶ KLæ•£åº¦: {avg_kl:.4f}")
        print(f"   â€¢ Vç»„ä»¶ ç±»åˆ«åˆ†ç¦»åº¦: {avg_separation:.4f}")
        print(f"   â€¢ Mç»„ä»¶ ä¸»å¯¼åˆ†é‡æ¦‚ç‡: {dominant_prob:.3f}")
        print(f"   â€¢ Mç»„ä»¶ æ³¨æ„åŠ›ç†µ: {avg_attention_entropy:.3f}")
        
        # è¾“å‡ºç²¾ç®€ç‰ˆæŠ¥å‘Š
        print("\n" + "="*60)
        print("ğŸ“‹ å¿«é€Ÿè¯„ä¼°æ€»ç»“:")
        print(f"   Vç»„ä»¶ç¼–ç è´¨é‡: {'ä¼˜ç§€' if avg_kl < 5.0 and avg_separation > 2.0 else 'è‰¯å¥½' if avg_kl < 8.0 else 'å¯æ¥å—'}")
        print(f"   Mç»„ä»¶è®°å¿†æ•ˆç‡: {'é«˜æ•ˆ' if avg_attention_entropy < 2.5 and dominant_prob < 0.4 else 'æœ‰æ•ˆ' if avg_attention_entropy < 3.0 else 'å¯æ¥å—'}")
        print(f"   æ•´ä½“æ¶æ„è¡¨ç°: {'ä¼˜ç§€' if avg_kl < 6.0 and avg_attention_entropy < 2.8 else 'è‰¯å¥½'}")
        print("="*60)
        
    except Exception as e:
        print(f"âŒ å¯è§†åŒ–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    final_visualize_components()
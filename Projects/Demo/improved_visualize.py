#!/usr/bin/env python3
"""
æ”¹è¿›çš„VMCå¯è§†åŒ–è„šæœ¬ - æ ¹æ®ç”¨æˆ·éœ€æ±‚å®šåˆ¶
"""
import matplotlib
matplotlib.use('Agg')  # å¼ºåˆ¶ä½¿ç”¨éäº¤äº’å¼åç«¯

import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.metrics import mean_squared_error
import os
from config import VMCConfig
from vmc_model import create_vmc_model
from dataset import VMCDataset

def calculate_psnr(original, reconstructed):
    """è®¡ç®—PSNR (Peak Signal-to-Noise Ratio)"""
    mse = np.mean((original - reconstructed) ** 2)
    if mse == 0:
        return float('inf')
    max_pixel = 1.0
    psnr = 20 * np.log10(max_pixel / np.sqrt(mse))
    return psnr

def calculate_ssim_simple(img1, img2):
    """ç®€åŒ–çš„SSIMè®¡ç®—"""
    mu1 = np.mean(img1)
    mu2 = np.mean(img2)
    sigma1_sq = np.var(img1)
    sigma2_sq = np.var(img2)
    sigma12 = np.mean((img1 - mu1) * (img2 - mu2))
    
    c1 = 0.01 ** 2
    c2 = 0.03 ** 2
    
    ssim = ((2 * mu1 * mu2 + c1) * (2 * sigma12 + c2)) / ((mu1**2 + mu2**2 + c1) * (sigma1_sq + sigma2_sq + c2))
    return ssim

def improved_visualize_components():
    """æ”¹è¿›çš„VMCç»„ä»¶å¯è§†åŒ–"""
    
    print("ğŸ¨ æ”¹è¿›çš„VMCå¯è§†åŒ–å¼€å§‹...")
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs('results', exist_ok=True)
    
    try:
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint_path = "checkpoints/vmc_checkpoint_epoch_3.pth"
        print(f"ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=VMCConfig.device, weights_only=False)
        
        # åˆ›å»ºæ¨¡å‹å¹¶åŠ è½½æƒé‡
        model = create_vmc_model(VMCConfig())
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # åŠ è½½æ•°æ®
        dataset = VMCDataset()
        train_loader, test_loader = dataset.create_dataloaders()
        
        # è·å–ä¸€äº›æµ‹è¯•æ•°æ®
        with torch.no_grad():
            for images, labels in test_loader:
                images = images.to(VMCConfig.device)
                labels = labels.to(VMCConfig.device)
                
                # æ‰å¹³åŒ–å›¾åƒ [batch_size, 1, 28, 28] -> [batch_size, 784]
                images_flat = images.view(images.size(0), -1)
                
                # é€šè¿‡æ¨¡å‹è·å–å„ç»„ä»¶è¾“å‡º
                # Vç»„ä»¶ - å˜åˆ†ç¼–ç å™¨
                v_output, mu, logvar = model.variational_encoder(images_flat)
                
                # é‡æ„å›¾åƒ (é€šè¿‡è§£ç å™¨)
                # åˆ›å»ºä¸€ä¸ªç®€å•çš„è§£ç å™¨æ¥é‡æ„å›¾åƒç”¨äºè¯„ä¼°
                decoder = torch.nn.Sequential(
                    torch.nn.Linear(VMCConfig.variational_dim, 256),
                    torch.nn.ReLU(),
                    torch.nn.Linear(256, 512),
                    torch.nn.ReLU(),
                    torch.nn.Linear(512, 784),
                    torch.nn.Sigmoid()
                ).to(VMCConfig.device)
                
                # ä½¿ç”¨å˜åˆ†ç¼–ç è¿›è¡Œé‡æ„
                reconstructed_flat = decoder(v_output)
                reconstructed = reconstructed_flat.view(-1, 1, 28, 28)
                
                # å°†å˜åˆ†ç¼–ç è½¬æ¢ä¸ºè®°å¿†æŸ¥è¯¢ (64 -> 32ç»´)
                memory_query = model.var_to_memory(v_output)
                
                # Mç»„ä»¶ - è®°å¿†æ¨¡å—  
                memory_output, attention_weights, mixture_probs = model.memory_module(memory_query)
                
                break  # åªå¤„ç†ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„ä»¥ä¾¿å¯è§†åŒ–
        images_np = images.cpu().numpy()
        reconstructed_np = reconstructed.cpu().numpy()
        v_data = v_output.cpu().numpy()
        mu_np = mu.cpu().numpy()
        logvar_np = logvar.cpu().numpy()
        attention_data = attention_weights.cpu().numpy()
        mixture_data = mixture_probs.cpu().numpy()
        labels_np = labels.cpu().numpy()
        
        print("ğŸ“Š å¼€å§‹ç”Ÿæˆæ”¹è¿›çš„å¯è§†åŒ–å›¾è¡¨...")
        
        # 1. Vç»„ä»¶å¯è§†åŒ– - é‡æ„å›¾åƒè´¨é‡è¯„ä¼°
        print("   ç”ŸæˆVç»„ä»¶é‡æ„è´¨é‡è¯„ä¼°...")
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        
        # åŸå›¾vsé‡æ„å›¾å¯¹æ¯” (æ˜¾ç¤ºå‰6ä¸ªæ ·æœ¬)
        for i in range(6):
            row = i // 3
            col = i % 3
            
            # åŸå›¾
            axes[row, col].imshow(images_np[i, 0], cmap='gray')
            axes[row, col].set_title(f'Original vs Reconstructed\nSample {i+1} (Label: {labels_np[i]})')
            axes[row, col].axis('off')
            
            # åœ¨åŒä¸€ä¸ªsubplotä¸­æ˜¾ç¤ºé‡æ„å›¾ï¼ˆä½¿ç”¨è¾¹æ¡†åŒºåˆ†ï¼‰
            # åˆ›å»ºä¸€ä¸ªæ‹¼æ¥å›¾åƒï¼šå·¦è¾¹åŸå›¾ï¼Œå³è¾¹é‡æ„å›¾
            combined = np.hstack([images_np[i, 0], reconstructed_np[i, 0]])
            axes[row, col].clear()
            axes[row, col].imshow(combined, cmap='gray')
            axes[row, col].axvline(x=13.5, color='red', linewidth=2)  # åˆ†å‰²çº¿
            axes[row, col].set_title(f'Original | Reconstructed\nLabel: {labels_np[i]}')
            axes[row, col].axis('off')
        
        plt.tight_layout()
        plt.savefig('results/v_component_reconstruction_comparison.png', dpi=150, bbox_inches='tight')
        plt.close()
        
        # Vç»„ä»¶è´¨é‡æŒ‡æ ‡
        plt.figure(figsize=(15, 5))
        
        # è®¡ç®—é‡æ„è´¨é‡æŒ‡æ ‡
        reconstruction_metrics = []
        psnr_values = []
        ssim_values = []
        mse_values = []
        
        for i in range(min(32, len(images_np))):  # è®¡ç®—å‰32ä¸ªæ ·æœ¬çš„æŒ‡æ ‡
            original = images_np[i, 0]
            recon = reconstructed_np[i, 0]
            
            mse = mean_squared_error(original.flatten(), recon.flatten())
            psnr = calculate_psnr(original, recon)
            ssim = calculate_ssim_simple(original, recon)
            
            mse_values.append(mse)
            psnr_values.append(psnr)
            ssim_values.append(ssim)
        
        # ç»˜åˆ¶è´¨é‡æŒ‡æ ‡
        plt.subplot(1, 4, 1)
        plt.hist(mse_values, bins=15, alpha=0.7, color='red')
        plt.title('Reconstruction MSE Distribution')
        plt.xlabel('MSE')
        plt.ylabel('Frequency')
        plt.axvline(np.mean(mse_values), color='darkred', linestyle='--', 
                   label=f'Mean: {np.mean(mse_values):.4f}')
        plt.legend()
        
        plt.subplot(1, 4, 2)
        plt.hist(psnr_values, bins=15, alpha=0.7, color='blue')
        plt.title('PSNR Distribution')
        plt.xlabel('PSNR (dB)')
        plt.ylabel('Frequency')
        plt.axvline(np.mean(psnr_values), color='darkblue', linestyle='--',
                   label=f'Mean: {np.mean(psnr_values):.2f}dB')
        plt.legend()
        
        plt.subplot(1, 4, 3)
        plt.hist(ssim_values, bins=15, alpha=0.7, color='green')
        plt.title('SSIM Distribution')
        plt.xlabel('SSIM')
        plt.ylabel('Frequency')
        plt.axvline(np.mean(ssim_values), color='darkgreen', linestyle='--',
                   label=f'Mean: {np.mean(ssim_values):.3f}')
        plt.legend()
        
        # KLæ•£åº¦åˆ†å¸ƒ
        plt.subplot(1, 4, 4)
        kl_div = -0.5 * torch.sum(1 + torch.tensor(logvar_np) - torch.tensor(mu_np).pow(2) - torch.tensor(logvar_np).exp(), dim=1)
        plt.hist(kl_div.numpy(), bins=15, alpha=0.7, color='purple')
        plt.title('KL Divergence Distribution')
        plt.xlabel('KL Divergence')
        plt.ylabel('Frequency')
        plt.axvline(np.mean(kl_div.numpy()), color='indigo', linestyle='--',
                   label=f'Mean: {np.mean(kl_div.numpy()):.3f}')
        plt.legend()
        
        plt.tight_layout()
        plt.savefig('results/v_component_quality_metrics.png', dpi=150, bbox_inches='tight')
        plt.close()
        print("âœ… Vç»„ä»¶é‡æ„è´¨é‡è¯„ä¼°å®Œæˆ: results/v_component_reconstruction_comparison.png")
        print("âœ… Vç»„ä»¶è´¨é‡æŒ‡æ ‡å®Œæˆ: results/v_component_quality_metrics.png")
        
        # 2. Mç»„ä»¶å¯è§†åŒ– - æ¦‚ç‡åˆ†å¸ƒ
        print("   ç”ŸæˆMç»„ä»¶æ¦‚ç‡åˆ†å¸ƒ...")
        plt.figure(figsize=(15, 5))
        
        # æ··åˆé«˜æ–¯æ¦‚ç‡åˆ†å¸ƒ
        plt.subplot(1, 3, 1)
        # å¯¹æ¯ä¸ªæ ·æœ¬çš„æ··åˆæ¦‚ç‡æ±‚å¹³å‡
        avg_mixture_probs = np.mean(mixture_data, axis=0)
        plt.bar(range(len(avg_mixture_probs)), avg_mixture_probs, alpha=0.7, color='orange')
        plt.title('Mixture Component Probabilities\n(Average across batch)')
        plt.xlabel('Gaussian Component')
        plt.ylabel('Probability')
        plt.xticks(range(len(avg_mixture_probs)))
        for i, v in enumerate(avg_mixture_probs):
            plt.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')
        
        # æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒ
        plt.subplot(1, 3, 2)
        # è®¡ç®—å¹³å‡æ³¨æ„åŠ›æƒé‡
        avg_attention = np.mean(attention_data, axis=0)
        plt.bar(range(len(avg_attention)), avg_attention, alpha=0.7, color='cyan')
        plt.title('Memory Attention Weights\n(Average across batch)')
        plt.xlabel('Memory Slot')
        plt.ylabel('Attention Weight')
        plt.xticks(range(0, len(avg_attention), 2))  # åªæ˜¾ç¤ºéƒ¨åˆ†åˆ»åº¦
        
        # æ³¨æ„åŠ›æƒé‡çš„åˆ†å¸ƒæƒ…å†µ
        plt.subplot(1, 3, 3)
        attention_entropy = []
        for i in range(len(attention_data)):
            # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æ³¨æ„åŠ›ç†µï¼ˆè¡¡é‡æ³¨æ„åŠ›çš„é›†ä¸­ç¨‹åº¦ï¼‰
            attention = attention_data[i]
            attention = attention / np.sum(attention)  # å½’ä¸€åŒ–
            entropy = -np.sum(attention * np.log(attention + 1e-8))
            attention_entropy.append(entropy)
        
        plt.hist(attention_entropy, bins=15, alpha=0.7, color='magenta')
        plt.title('Attention Entropy Distribution\n(Higher = More Distributed)')
        plt.xlabel('Entropy')
        plt.ylabel('Frequency')
        plt.axvline(np.mean(attention_entropy), color='darkmagenta', linestyle='--',
                   label=f'Mean: {np.mean(attention_entropy):.3f}')
        plt.legend()
        
        plt.tight_layout()
        plt.savefig('results/m_component_probability_distributions.png', dpi=150, bbox_inches='tight')
        plt.close()
        print("âœ… Mç»„ä»¶æ¦‚ç‡åˆ†å¸ƒå®Œæˆ: results/m_component_probability_distributions.png")
        
        # 3. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        print("   ç”ŸæˆVMCæ€§èƒ½æ€»ç»“æŠ¥å‘Š...")
        
        # è®¡ç®—æ€»ä½“æ€§èƒ½æŒ‡æ ‡
        avg_mse = np.mean(mse_values)
        avg_psnr = np.mean(psnr_values)
        avg_ssim = np.mean(ssim_values)
        avg_kl = np.mean(kl_div.numpy())
        avg_attention_entropy = np.mean(attention_entropy)
        
        # åˆ›å»ºæ€§èƒ½æŠ¥å‘Šæ–‡æœ¬
        report = f"""
VMC (Variational Memory Compression) æ€§èƒ½è¯„ä¼°æŠ¥å‘Š
===============================================

è®­ç»ƒä¿¡æ¯:
- æ£€æŸ¥ç‚¹: {checkpoint_path}
- è®­ç»ƒè½®æ•°: 3 epochs
- æ¨¡å‹å‚æ•°: 620,819

Vç»„ä»¶ (å˜åˆ†ç¼–ç å™¨) é‡æ„è´¨é‡è¯„ä¼°:
- å¹³å‡MSEæŸå¤±: {avg_mse:.6f}
- å¹³å‡PSNR: {avg_psnr:.2f} dB
- å¹³å‡SSIM: {avg_ssim:.4f}
- å¹³å‡KLæ•£åº¦: {avg_kl:.4f}

é‡æ„è´¨é‡è¯„çº§:
- PSNR > 20dB: {'ä¼˜ç§€' if avg_psnr > 20 else 'è‰¯å¥½' if avg_psnr > 15 else 'ä¸€èˆ¬'}
- SSIM > 0.8: {'ä¼˜ç§€' if avg_ssim > 0.8 else 'è‰¯å¥½' if avg_ssim > 0.6 else 'ä¸€èˆ¬'}

Mç»„ä»¶ (è®°å¿†æ¨¡å—) æ¦‚ç‡åˆ†å¸ƒåˆ†æ:
- æ··åˆé«˜æ–¯ç»„ä»¶æ•°: {len(avg_mixture_probs)}
- æœ€æ´»è·ƒç»„ä»¶: Component {np.argmax(avg_mixture_probs)} (æ¦‚ç‡: {np.max(avg_mixture_probs):.3f})
- è®°å¿†æ§½æ•°é‡: {len(avg_attention)}
- å¹³å‡æ³¨æ„åŠ›ç†µ: {avg_attention_entropy:.3f}

æ³¨æ„åŠ›é›†ä¸­åº¦è¯„ä¼°:
- ç†µå€¼ < 2.0: {'é›†ä¸­' if avg_attention_entropy < 2.0 else 'åˆ†æ•£'}

ç”Ÿæˆçš„å¯è§†åŒ–æ–‡ä»¶:
1. results/v_component_reconstruction_comparison.png - åŸå›¾vsé‡æ„å›¾å¯¹æ¯”
2. results/v_component_quality_metrics.png - é‡æ„è´¨é‡æŒ‡æ ‡åˆ†å¸ƒ  
3. results/m_component_probability_distributions.png - è®°å¿†æ¨¡å—æ¦‚ç‡åˆ†å¸ƒ

æ€»ä½“è¯„ä¼°: VMCæ¨¡å‹å±•ç°äº†{'è‰¯å¥½' if avg_psnr > 15 and avg_ssim > 0.6 else 'å¯æ¥å—'}çš„é‡æ„æ€§èƒ½å’Œ{'æœ‰æ•ˆ' if avg_attention_entropy < 3.0 else 'åˆ†æ•£'}çš„è®°å¿†æœºåˆ¶ã€‚
"""
        
        # ä¿å­˜æŠ¥å‘Š
        with open('results/vmc_performance_report.txt', 'w', encoding='utf-8') as f:
            f.write(report)
        
        print("\nğŸ‰ æ”¹è¿›çš„VMCå¯è§†åŒ–å…¨éƒ¨å®Œæˆ!")
        print("ğŸ“Š ç”Ÿæˆçš„æ–‡ä»¶:")
        print("   â€¢ results/v_component_reconstruction_comparison.png - Vç»„ä»¶é‡æ„å¯¹æ¯”")
        print("   â€¢ results/v_component_quality_metrics.png - Vç»„ä»¶è´¨é‡æŒ‡æ ‡")
        print("   â€¢ results/m_component_probability_distributions.png - Mç»„ä»¶æ¦‚ç‡åˆ†å¸ƒ")
        print("   â€¢ results/vmc_performance_report.txt - æ€§èƒ½è¯„ä¼°æŠ¥å‘Š")
        print(f"\nğŸ“ˆ å…³é”®æŒ‡æ ‡:")
        print(f"   â€¢ å¹³å‡PSNR: {avg_psnr:.2f} dB")
        print(f"   â€¢ å¹³å‡SSIM: {avg_ssim:.4f}")
        print(f"   â€¢ å¹³å‡æ³¨æ„åŠ›ç†µ: {avg_attention_entropy:.3f}")
        
        # è¾“å‡ºæŠ¥å‘Šå†…å®¹
        print("\n" + "="*50)
        print(report)
        
    except Exception as e:
        print(f"âŒ å¯è§†åŒ–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    improved_visualize_components()
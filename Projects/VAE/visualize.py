"""
å¯è§†åŒ–æ¨¡å— - VAEç»“æœå¯è§†åŒ–å’Œåˆ†æ
"""
import torch
import numpy as np
import matplotlib.pyplot as plt
from torchvision.utils import make_grid
import os
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

from config import Config
from vae import create_vae_model
from dataset import create_dataloader

class VAEVisualizer:
    """VAEå¯è§†åŒ–å™¨"""
    
    def __init__(self, model=None, dataloader=None):
        """
        åˆå§‹åŒ–å¯è§†åŒ–å™¨
        
        Args:
            model: è®­ç»ƒå¥½çš„VAEæ¨¡å‹
            dataloader: æ•°æ®åŠ è½½å™¨
        """
        self.device = Config.device
        self.model = model if model is not None else create_vae_model()
        self.dataloader = dataloader if dataloader is not None else create_dataloader()
        
        if self.dataloader is None:
            raise ValueError("æ— æ³•åˆ›å»ºæ•°æ®åŠ è½½å™¨")
            
        self.model.eval()
        print("ğŸ¨ VAEå¯è§†åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def visualize_latent_space(self, num_samples=1000, method='pca', save_path="results/latent_space.png"):
        """
        å¯è§†åŒ–æ½œåœ¨ç©ºé—´åˆ†å¸ƒ
        
        Args:
            num_samples: é‡‡æ ·æ•°é‡
            method: é™ç»´æ–¹æ³• ('pca' æˆ– 'tsne')
            save_path: ä¿å­˜è·¯å¾„
        """
        print(f"ğŸ“Š å¯è§†åŒ–æ½œåœ¨ç©ºé—´åˆ†å¸ƒ (æ–¹æ³•: {method})")
        
        # æ”¶é›†æ½œåœ¨ç©ºé—´ç¼–ç 
        mu_list = []
        
        with torch.no_grad():
            sample_count = 0
            for x, _ in self.dataloader:
                if sample_count >= num_samples:
                    break
                    
                x = x.to(self.device)
                mu, _ = self.model.encoder(x)
                mu_list.append(mu.cpu())
                sample_count += x.size(0)
        
        # åˆå¹¶æ‰€æœ‰ç¼–ç 
        mu_all = torch.cat(mu_list)[:num_samples].numpy()
        print(f"   æ”¶é›†äº† {mu_all.shape[0]} ä¸ªæ ·æœ¬çš„æ½œåœ¨ç¼–ç ")
        
        # é™ç»´åˆ°2D
        if method == 'pca':
            reducer = PCA(n_components=2)
            mu_2d = reducer.fit_transform(mu_all)
            title = f"Latent Space (PCA) - {mu_all.shape[1]}D â†’ 2D"
        elif method == 'tsne':
            reducer = TSNE(n_components=2, random_state=42, perplexity=30)
            mu_2d = reducer.fit_transform(mu_all)
            title = f"Latent Space (t-SNE) - {mu_all.shape[1]}D â†’ 2D"
        else:
            # å¦‚æœæ½œåœ¨ç©ºé—´ç»´åº¦>=2ï¼Œç›´æ¥ä½¿ç”¨å‰ä¸¤ä¸ªç»´åº¦
            mu_2d = mu_all[:, :2]
            title = "Latent Space (First 2 Dimensions)"
        
        # åˆ›å»ºæ•£ç‚¹å›¾
        plt.figure(figsize=(10, 8))
        plt.scatter(mu_2d[:, 0], mu_2d[:, 1], alpha=0.6, s=1, c='blue')
        plt.title(title)
        plt.xlabel("Dimension 1")
        plt.ylabel("Dimension 2")
        plt.grid(True, alpha=0.3)
        
        # ä¿å­˜å›¾åƒ
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.show()
        
        print(f"   ğŸ’¾ æ½œåœ¨ç©ºé—´å¯è§†åŒ–å·²ä¿å­˜: {save_path}")
    
    def generate_samples(self, num_samples=16, grid_size=4, save_path="results/generated_samples.png"):
        """
        ç”Ÿæˆå¹¶å¯è§†åŒ–æ ·æœ¬
        
        Args:
            num_samples: ç”Ÿæˆæ ·æœ¬æ•°é‡
            grid_size: ç½‘æ ¼å¤§å°
            save_path: ä¿å­˜è·¯å¾„
        """
        print(f"ğŸ­ ç”Ÿæˆ {num_samples} ä¸ªæ ·æœ¬")
        
        with torch.no_grad():
            # ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒé‡‡æ ·
            z = torch.randn(num_samples, self.model.latent_dim).to(self.device)
            samples = self.model.decoder(z)
            
            # åå½’ä¸€åŒ–åˆ°[0,1]
            samples = (samples + 1) / 2
            samples = torch.clamp(samples, 0, 1)
            
            # åˆ›å»ºç½‘æ ¼
            grid = make_grid(samples, nrow=grid_size, pad_value=1)
            
            # æ˜¾ç¤ºå’Œä¿å­˜
            plt.figure(figsize=(10, 10))
            plt.imshow(grid.permute(1, 2, 0).cpu())
            plt.axis('off')
            plt.title('Generated Faces from Random Latent Codes')
            
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            plt.show()
            
            print(f"   ğŸ’¾ ç”Ÿæˆæ ·æœ¬å·²ä¿å­˜: {save_path}")
    
    def show_reconstruction(self, num_pairs=8, save_path="results/reconstruction.png"):
        """
        æ˜¾ç¤ºé‡å»ºå¯¹æ¯”
        
        Args:
            num_pairs: å¯¹æ¯”å¯¹æ•°
            save_path: ä¿å­˜è·¯å¾„
        """
        print(f"ğŸ”„ æ˜¾ç¤ºé‡å»ºå¯¹æ¯” ({num_pairs} å¯¹)")
        
        with torch.no_grad():
            # è·å–çœŸå®å›¾åƒ
            x, _ = next(iter(self.dataloader))
            x = x[:num_pairs].to(self.device)
            
            # é‡å»ºå›¾åƒ
            x_hat, _, _ = self.model(x)
            
            # åå½’ä¸€åŒ–
            x_display = (x.cpu() + 1) / 2
            x_hat_display = (x_hat.cpu() + 1) / 2
            x_display = torch.clamp(x_display, 0, 1)
            x_hat_display = torch.clamp(x_hat_display, 0, 1)
            
            # äº¤æ›¿æ’åˆ—åŸå›¾å’Œé‡å»ºå›¾
            comparison = torch.zeros(2 * num_pairs, 3, Config.image_size, Config.image_size)
            for i in range(num_pairs):
                comparison[2*i] = x_display[i]      # åŸå›¾
                comparison[2*i+1] = x_hat_display[i]  # é‡å»ºå›¾
            
            # åˆ›å»ºç½‘æ ¼
            grid = make_grid(comparison, nrow=2, pad_value=1)
            
            # æ˜¾ç¤ºå’Œä¿å­˜
            plt.figure(figsize=(15, 2 * num_pairs))
            plt.imshow(grid.permute(1, 2, 0))
            plt.axis('off')
            plt.title('Reconstruction Comparison (Original | Reconstructed)')
            
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            plt.show()
            
            print(f"   ğŸ’¾ é‡å»ºå¯¹æ¯”å·²ä¿å­˜: {save_path}")
    
    def latent_space_interpolation(self, num_steps=10, save_path="results/interpolation.png"):
        """
        æ½œåœ¨ç©ºé—´æ’å€¼å¯è§†åŒ–
        
        Args:
            num_steps: æ’å€¼æ­¥æ•°
            save_path: ä¿å­˜è·¯å¾„
        """
        print(f"ğŸŒˆ æ½œåœ¨ç©ºé—´æ’å€¼ ({num_steps} æ­¥)")
        
        with torch.no_grad():
            # éšæœºé€‰æ‹©ä¸¤ä¸ªæ½œåœ¨ç‚¹
            z1 = torch.randn(1, self.model.latent_dim).to(self.device)
            z2 = torch.randn(1, self.model.latent_dim).to(self.device)
            
            # çº¿æ€§æ’å€¼
            alphas = torch.linspace(0, 1, num_steps).to(self.device)
            interpolated_samples = []
            
            for alpha in alphas:
                z_interp = (1 - alpha) * z1 + alpha * z2
                sample = self.model.decoder(z_interp)
                interpolated_samples.append(sample)
            
            # åˆå¹¶æ‰€æœ‰æ’å€¼æ ·æœ¬
            samples = torch.cat(interpolated_samples, dim=0)
            
            # åå½’ä¸€åŒ–
            samples = (samples + 1) / 2
            samples = torch.clamp(samples, 0, 1)
            
            # åˆ›å»ºç½‘æ ¼
            grid = make_grid(samples, nrow=num_steps, pad_value=1)
            
            # æ˜¾ç¤ºå’Œä¿å­˜
            plt.figure(figsize=(2 * num_steps, 4))
            plt.imshow(grid.permute(1, 2, 0).cpu())
            plt.axis('off')
            plt.title('Latent Space Interpolation')
            
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            plt.show()
            
            print(f"   ğŸ’¾ æ’å€¼ç»“æœå·²ä¿å­˜: {save_path}")
    
    def plot_training_curves(self, train_losses, recon_losses, kl_losses, save_path="results/training_curves.png"):
        """
        ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        
        Args:
            train_losses: æ€»è®­ç»ƒæŸå¤±
            recon_losses: é‡å»ºæŸå¤±
            kl_losses: KLæŸå¤±
            save_path: ä¿å­˜è·¯å¾„
        """
        print("ğŸ“ˆ ç»˜åˆ¶è®­ç»ƒæ›²çº¿")
        
        epochs = range(1, len(train_losses) + 1)
        
        plt.figure(figsize=(15, 5))
        
        # æ€»æŸå¤±
        plt.subplot(1, 3, 1)
        plt.plot(epochs, train_losses, 'b-', label='Total Loss')
        plt.title('Total Training Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        
        # é‡å»ºæŸå¤±
        plt.subplot(1, 3, 2)
        plt.plot(epochs, recon_losses, 'r-', label='Reconstruction Loss')
        plt.title('Reconstruction Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        
        # KLæŸå¤±
        plt.subplot(1, 3, 3)
        plt.plot(epochs, kl_losses, 'g-', label='KL Loss')
        plt.title('KL Divergence Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        
        plt.tight_layout()
        
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.show()
        
        print(f"   ğŸ’¾ è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {save_path}")
    
    def comprehensive_analysis(self, results_dir="results"):
        """
        ç»¼åˆåˆ†æå’Œå¯è§†åŒ–
        
        Args:
            results_dir: ç»“æœä¿å­˜ç›®å½•
        """
        print("ğŸ” å¼€å§‹ç»¼åˆåˆ†æ...")
        
        os.makedirs(results_dir, exist_ok=True)
        
        # 1. æ½œåœ¨ç©ºé—´å¯è§†åŒ–
        self.visualize_latent_space(
            save_path=os.path.join(results_dir, "latent_space_pca.png"),
            method='pca'
        )
        
        # 2. ç”Ÿæˆæ ·æœ¬
        self.generate_samples(
            save_path=os.path.join(results_dir, "generated_samples.png")
        )
        
        # 3. é‡å»ºå¯¹æ¯”
        self.show_reconstruction(
            save_path=os.path.join(results_dir, "reconstruction_comparison.png")
        )
        
        # 4. æ½œåœ¨ç©ºé—´æ’å€¼
        self.latent_space_interpolation(
            save_path=os.path.join(results_dir, "latent_interpolation.png")
        )
        
        print(f"âœ… ç»¼åˆåˆ†æå®Œæˆï¼Œç»“æœä¿å­˜åœ¨: {results_dir}")

def load_and_visualize(model_path="vae_model.pth"):
    """
    åŠ è½½æ¨¡å‹å¹¶è¿›è¡Œå¯è§†åŒ–
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
    """
    # åˆ›å»ºæ¨¡å‹
    model = create_vae_model()
    
    # åŠ è½½æƒé‡
    checkpoint = torch.load(model_path, map_location=Config.device)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    print(f"ğŸ“‚ å·²åŠ è½½æ¨¡å‹: {model_path}")
    
    # åˆ›å»ºå¯è§†åŒ–å™¨
    visualizer = VAEVisualizer(model)
    
    # ç»¼åˆåˆ†æ
    visualizer.comprehensive_analysis()
    
    # å¦‚æœæœ‰è®­ç»ƒå†å²ï¼Œç»˜åˆ¶è®­ç»ƒæ›²çº¿
    if 'train_losses' in checkpoint:
        visualizer.plot_training_curves(
            checkpoint['train_losses'],
            checkpoint['recon_losses'], 
            checkpoint['kl_losses']
        )

if __name__ == "__main__":
    print("å¯åŠ¨VAEå¯è§†åŒ–...")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹
    if os.path.exists("vae_model.pth"):
        load_and_visualize("vae_model.pth")
    else:
        print("æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒ")
        print("æ‚¨å¯ä»¥è¿è¡Œ: python train.py")
"""
VAEæ¨¡å‹æ¶æ„ - ç¼–ç å™¨ã€è§£ç å™¨å’ŒVAEç±»
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from config import Config

class Encoder(nn.Module):
    """VAEç¼–ç å™¨ - å°†å›¾åƒç¼–ç ä¸ºæ½œåœ¨ç©ºé—´çš„å‡å€¼å’Œæ–¹å·®"""
    
    def __init__(self, latent_dim=None):
        super().__init__()
        if latent_dim is None:
            latent_dim = Config.latent_dim
            
        # å·ç§¯å±‚ï¼š3x64x64 -> 256x4x4
        self.conv1 = nn.Conv2d(3, 32, 4, 2, 1)      # 32x32x32
        self.conv2 = nn.Conv2d(32, 64, 4, 2, 1)     # 64x16x16
        self.conv3 = nn.Conv2d(64, 128, 4, 2, 1)    # 128x8x8
        self.conv4 = nn.Conv2d(128, 256, 4, 2, 1)   # 256x4x4
        
        # å…¨è¿æ¥å±‚ï¼šè¾“å‡ºæ½œåœ¨ç©ºé—´çš„å‡å€¼å’Œæ–¹å·®
        self.fc_mu = nn.Linear(256 * 4 * 4, latent_dim)
        self.fc_logvar = nn.Linear(256 * 4 * 4, latent_dim)

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥å›¾åƒ (batch_size, 3, 64, 64)
            
        Returns:
            mu: æ½œåœ¨ç©ºé—´å‡å€¼ (batch_size, latent_dim)
            logvar: æ½œåœ¨ç©ºé—´å¯¹æ•°æ–¹å·® (batch_size, latent_dim)
        """
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = F.relu(self.conv4(x))
        
        x = x.view(x.size(0), -1)  # å±•å¹³
        mu = self.fc_mu(x)
        logvar = self.fc_logvar(x)
        return mu, logvar

class Decoder(nn.Module):
    """VAEè§£ç å™¨ - å°†æ½œåœ¨ç©ºé—´ç¼–ç é‡å»ºä¸ºå›¾åƒ"""
    
    def __init__(self, latent_dim=None):
        super().__init__()
        if latent_dim is None:
            latent_dim = Config.latent_dim
            
        # å…¨è¿æ¥å±‚ï¼šæ½œåœ¨ç©ºé—´ -> ç‰¹å¾å›¾
        self.fc = nn.Linear(latent_dim, 256 * 4 * 4)
        
        # åå·ç§¯å±‚ï¼š256x4x4 -> 3x64x64
        self.deconv1 = nn.ConvTranspose2d(256, 128, 4, 2, 1)  # 128x8x8
        self.deconv2 = nn.ConvTranspose2d(128, 64, 4, 2, 1)   # 64x16x16
        self.deconv3 = nn.ConvTranspose2d(64, 32, 4, 2, 1)    # 32x32x32
        self.deconv4 = nn.ConvTranspose2d(32, 3, 4, 2, 1)     # 3x64x64

    def forward(self, z):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            z: æ½œåœ¨ç©ºé—´ç¼–ç  (batch_size, latent_dim)
            
        Returns:
            x_hat: é‡å»ºå›¾åƒ (batch_size, 3, 64, 64)
        """
        x = F.relu(self.fc(z))
        x = x.view(x.size(0), 256, 4, 4)  # é‡å¡‘ä¸º4Då¼ é‡
        
        x = F.relu(self.deconv1(x))
        x = F.relu(self.deconv2(x))
        x = F.relu(self.deconv3(x))
        x = torch.tanh(self.deconv4(x))  # ä½¿ç”¨tanhé…åˆ[-1,1]å½’ä¸€åŒ–
        return x

class VAE(nn.Module):
    """å˜åˆ†è‡ªç¼–ç å™¨(VAE)ä¸»æ¨¡å‹"""
    
    def __init__(self, latent_dim=None):
        super().__init__()
        if latent_dim is None:
            latent_dim = Config.latent_dim
            
        self.latent_dim = latent_dim
        self.encoder = Encoder(latent_dim)
        self.decoder = Decoder(latent_dim)

    def reparameterize(self, mu, logvar):
        """
        é‡å‚æ•°åŒ–æŠ€å·§ï¼šä»N(mu, var)é‡‡æ ·
        
        Args:
            mu: å‡å€¼
            logvar: å¯¹æ•°æ–¹å·®
            
        Returns:
            z: é‡å‚æ•°åŒ–åçš„æ½œåœ¨å˜é‡
        """
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)  # æ ‡å‡†æ­£æ€åˆ†å¸ƒå™ªå£°
        return mu + eps * std

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥å›¾åƒ (batch_size, 3, 64, 64)
            
        Returns:
            x_hat: é‡å»ºå›¾åƒ (batch_size, 3, 64, 64)
            mu: æ½œåœ¨ç©ºé—´å‡å€¼ (batch_size, latent_dim) 
            logvar: æ½œåœ¨ç©ºé—´å¯¹æ•°æ–¹å·® (batch_size, latent_dim)
        """
        mu, logvar = self.encoder(x)
        z = self.reparameterize(mu, logvar)
        x_hat = self.decoder(z)
        return x_hat, mu, logvar
    
    def sample(self, num_samples, device=None):
        """
        ä»æ½œåœ¨ç©ºé—´é‡‡æ ·ç”Ÿæˆæ–°å›¾åƒ
        
        Args:
            num_samples: é‡‡æ ·æ•°é‡
            device: è®¾å¤‡
            
        Returns:
            ç”Ÿæˆçš„å›¾åƒ
        """
        if device is None:
            device = Config.device
            
        self.eval()
        with torch.no_grad():
            z = torch.randn(num_samples, self.latent_dim).to(device)
            samples = self.decoder(z)
            return samples

def vae_loss_fn(x_hat, x, mu, logvar, beta=1.0):
    """
    VAEæŸå¤±å‡½æ•°
    
    Args:
        x_hat: é‡å»ºå›¾åƒ
        x: åŸå§‹å›¾åƒ
        mu: æ½œåœ¨ç©ºé—´å‡å€¼
        logvar: æ½œåœ¨ç©ºé—´å¯¹æ•°æ–¹å·®
        beta: KLæŸå¤±æƒé‡ (Beta-VAE)
        
    Returns:
        total_loss: æ€»æŸå¤±
        recon_loss: é‡å»ºæŸå¤±
        kl_loss: KLæ•£åº¦æŸå¤±
    """
    # é‡å»ºæŸå¤± - ä½¿ç”¨MSE
    recon_loss = F.mse_loss(x_hat, x, reduction='sum')
    
    # KLæ•£åº¦æŸå¤±
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    
    # æ€»æŸå¤±
    total_loss = recon_loss + beta * kl_loss
    
    return total_loss, recon_loss, kl_loss

def create_vae_model(latent_dim=None):
    """
    åˆ›å»ºVAEæ¨¡å‹
    
    Args:
        latent_dim: æ½œåœ¨ç©ºé—´ç»´åº¦
        
    Returns:
        VAEæ¨¡å‹å®ä¾‹
    """
    if latent_dim is None:
        latent_dim = Config.latent_dim
    
    model = VAE(latent_dim).to(Config.device)
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"ğŸ—ï¸  VAEæ¨¡å‹åˆ›å»ºå®Œæˆ")
    print(f"   æ½œåœ¨ç©ºé—´ç»´åº¦: {latent_dim}")
    print(f"   æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    return model

if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å‹åˆ›å»º
    print("æµ‹è¯•VAEæ¨¡å‹...")
    vae = create_vae_model()
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    test_input = torch.randn(2, 3, 64, 64).to(Config.device)
    x_hat, mu, logvar = vae(test_input)
    
    print(f"è¾“å…¥å½¢çŠ¶: {test_input.shape}")
    print(f"é‡å»ºè¾“å‡ºå½¢çŠ¶: {x_hat.shape}")
    print(f"æ½œåœ¨å‡å€¼å½¢çŠ¶: {mu.shape}")
    print(f"æ½œåœ¨æ–¹å·®å½¢çŠ¶: {logvar.shape}")
    print("âœ… VAEæ¨¡å‹æµ‹è¯•é€šè¿‡ï¼")

